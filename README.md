# Evaluating LLMs on the Code Generation Task

This repository contains the code and text for my doctoral dissertation (Praxis) - Boosting the Code Generation Capabilities of Small Language Models (SLMs) Using Agents.

## Evaluation of Code Generated By LLMs
* Notebook `Evaluate_Generated_Code_Using_Pass@1.ipynb` for an explanation of how the code generation capabilities of LLMs are evaluated.
    - This notebook loads the code generated by an LLM and uses the modified human-eval package to evaluate how well the code was generated by computing the Pass@1 score. Pass@5 and Pass@10 scores can also be computed.
        * [Original package](https://github.com/openai/human-eval) - evaluates code generaion models only on the [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) dataset.
        * [My modifications](https://github.com/agnedil/code-generation/tree/main/modified-openai-human-eval-code) - I made an extensive addition to enable the evaluation on three more datasets: [MBPP](https://huggingface.co/datasets/google-research-datasets/mbpp), [LBPP](https://huggingface.co/datasets/CohereForAI/lbpp), and [Big Code Benchmark](https://huggingface.co/datasets/bigcode/bigcodebench)

## Structure of the Repository

See:
* Notebook `Visualizing Code Generation Results for Single Models.ipynb` for evaluating the code generation capabilities of single LLMs.
* Folder `notebooks` for all the code used in this research, so far.
* Folder `logs` for all the logs generated during the evaluation of generated code, so far.
* Folder `documents` for all supporting documents used to write the Praxis along with the main Praxis document.
* Folder `modified-openai-human-eval-code` for the modified human-eval package used to evaluate code on four code generation datasets.